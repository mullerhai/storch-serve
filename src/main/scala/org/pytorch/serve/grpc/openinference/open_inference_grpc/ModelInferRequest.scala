// Generated by the Scala Plugin for the Protocol Buffer Compiler.
// Do not edit!

package org.pytorch.serve.grpc.openinference.open_inference_grpc

/** @param modelName
  *   The name of the model to use for inferencing.
  * @param modelVersion
  *   The version of the model to use for inference. If not given the
  *   server will choose a version based on the model and internal policy.
  * @param id
  *   Optional identifier for the request. If specified will be
  *   returned in the response.
  * @param parameters
  *   Optional inference parameters.
  * @param inputs
  *   The input tensors for the inference.
  * @param outputs
  *   The requested output tensors for the inference. Optional, if not
  *   specified all outputs produced by the model will be returned.
  * @param rawInputContents
  *   The data contained in an input tensor can be represented in "raw"
  *   bytes form or in the repeated type that matches the tensor's data
  *   type. To use the raw representation 'raw_input_contents' must be
  *   initialized with data for each tensor in the same order as
  *   'inputs'. For each tensor, the size of this content must match
  *   what is expected by the tensor's shape and data type. The raw
  *   data must be the flattened, one-dimensional, row-major order of
  *   the tensor elements without any stride or padding between the
  *   elements. Note that the FP16 and BF16 data types must be represented as
  *   raw content as there is no specific data type for a 16-bit float type.
  *  
  *   If this field is specified then InferInputTensor::contents must
  *   not be specified for any input tensor.
  */
@SerialVersionUID(0L)
final case class ModelInferRequest(
    modelName: _root_.scala.Predef.String = "",
    modelVersion: _root_.scala.Predef.String = "",
    id: _root_.scala.Predef.String = "",
    parameters: _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter] = _root_.scala.collection.immutable.Map.empty,
    inputs: _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor] = _root_.scala.Seq.empty,
    outputs: _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor] = _root_.scala.Seq.empty,
    rawInputContents: _root_.scala.Seq[_root_.com.google.protobuf.ByteString] = _root_.scala.Seq.empty,
    unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
    ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[ModelInferRequest] {
    @transient
    private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
    private[this] def __computeSerializedSize(): _root_.scala.Int = {
      var __size = 0
      
      {
        val __value = modelName
        if (!__value.isEmpty) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(1, __value)
        }
      };
      
      {
        val __value = modelVersion
        if (!__value.isEmpty) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(2, __value)
        }
      };
      
      {
        val __value = id
        if (!__value.isEmpty) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(3, __value)
        }
      };
      parameters.foreach { __item =>
        val __value = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest._typemapper_parameters.toBase(__item)
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      }
      inputs.foreach { __item =>
        val __value = __item
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      }
      outputs.foreach { __item =>
        val __value = __item
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      }
      rawInputContents.foreach { __item =>
        val __value = __item
        __size += _root_.com.google.protobuf.CodedOutputStream.computeBytesSize(7, __value)
      }
      __size += unknownFields.serializedSize
      __size
    }
    override def serializedSize: _root_.scala.Int = {
      var __size = __serializedSizeMemoized
      if (__size == 0) {
        __size = __computeSerializedSize() + 1
        __serializedSizeMemoized = __size
      }
      __size - 1
      
    }
    def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
      {
        val __v = modelName
        if (!__v.isEmpty) {
          _output__.writeString(1, __v)
        }
      };
      {
        val __v = modelVersion
        if (!__v.isEmpty) {
          _output__.writeString(2, __v)
        }
      };
      {
        val __v = id
        if (!__v.isEmpty) {
          _output__.writeString(3, __v)
        }
      };
      parameters.foreach { __v =>
        val __m = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest._typemapper_parameters.toBase(__v)
        _output__.writeTag(4, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      inputs.foreach { __v =>
        val __m = __v
        _output__.writeTag(5, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      outputs.foreach { __v =>
        val __m = __v
        _output__.writeTag(6, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      rawInputContents.foreach { __v =>
        val __m = __v
        _output__.writeBytes(7, __m)
      };
      unknownFields.writeTo(_output__)
    }
    def withModelName(__v: _root_.scala.Predef.String): ModelInferRequest = copy(modelName = __v)
    def withModelVersion(__v: _root_.scala.Predef.String): ModelInferRequest = copy(modelVersion = __v)
    def withId(__v: _root_.scala.Predef.String): ModelInferRequest = copy(id = __v)
    def clearParameters = copy(parameters = _root_.scala.collection.immutable.Map.empty)
    def addParameters(__vs: (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter) *): ModelInferRequest = addAllParameters(__vs)
    def addAllParameters(__vs: Iterable[(_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)]): ModelInferRequest = copy(parameters = parameters ++ __vs)
    def withParameters(__v: _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]): ModelInferRequest = copy(parameters = __v)
    def clearInputs = copy(inputs = _root_.scala.Seq.empty)
    def addInputs(__vs: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor *): ModelInferRequest = addAllInputs(__vs)
    def addAllInputs(__vs: Iterable[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor]): ModelInferRequest = copy(inputs = inputs ++ __vs)
    def withInputs(__v: _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor]): ModelInferRequest = copy(inputs = __v)
    def clearOutputs = copy(outputs = _root_.scala.Seq.empty)
    def addOutputs(__vs: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor *): ModelInferRequest = addAllOutputs(__vs)
    def addAllOutputs(__vs: Iterable[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor]): ModelInferRequest = copy(outputs = outputs ++ __vs)
    def withOutputs(__v: _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor]): ModelInferRequest = copy(outputs = __v)
    def clearRawInputContents = copy(rawInputContents = _root_.scala.Seq.empty)
    def addRawInputContents(__vs: _root_.com.google.protobuf.ByteString *): ModelInferRequest = addAllRawInputContents(__vs)
    def addAllRawInputContents(__vs: Iterable[_root_.com.google.protobuf.ByteString]): ModelInferRequest = copy(rawInputContents = rawInputContents ++ __vs)
    def withRawInputContents(__v: _root_.scala.Seq[_root_.com.google.protobuf.ByteString]): ModelInferRequest = copy(rawInputContents = __v)
    def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
    def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
    def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
      (__fieldNumber: @_root_.scala.unchecked) match {
        case 1 => {
          val __t = modelName
          if (__t != "") __t else null
        }
        case 2 => {
          val __t = modelVersion
          if (__t != "") __t else null
        }
        case 3 => {
          val __t = id
          if (__t != "") __t else null
        }
        case 4 => parameters.iterator.map(org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest._typemapper_parameters.toBase(_)).toSeq
        case 5 => inputs
        case 6 => outputs
        case 7 => rawInputContents
      }
    }
    def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
      _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
      (__field.number: @_root_.scala.unchecked) match {
        case 1 => _root_.scalapb.descriptors.PString(modelName)
        case 2 => _root_.scalapb.descriptors.PString(modelVersion)
        case 3 => _root_.scalapb.descriptors.PString(id)
        case 4 => _root_.scalapb.descriptors.PRepeated(parameters.iterator.map(org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest._typemapper_parameters.toBase(_).toPMessage).toVector)
        case 5 => _root_.scalapb.descriptors.PRepeated(inputs.iterator.map(_.toPMessage).toVector)
        case 6 => _root_.scalapb.descriptors.PRepeated(outputs.iterator.map(_.toPMessage).toVector)
        case 7 => _root_.scalapb.descriptors.PRepeated(rawInputContents.iterator.map(_root_.scalapb.descriptors.PByteString(_)).toVector)
      }
    }
    def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
    def companion: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.type = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest
    // @@protoc_insertion_point(GeneratedMessage[org.pytorch.serve.grpc.openinference.ModelInferRequest])
}

object ModelInferRequest extends scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest] {
  implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest] = this
  def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest = {
    var __modelName: _root_.scala.Predef.String = ""
    var __modelVersion: _root_.scala.Predef.String = ""
    var __id: _root_.scala.Predef.String = ""
    val __parameters: _root_.scala.collection.mutable.Builder[(_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter), _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]] = _root_.scala.collection.immutable.Map.newBuilder[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]
    val __inputs: _root_.scala.collection.immutable.VectorBuilder[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor] = new _root_.scala.collection.immutable.VectorBuilder[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor]
    val __outputs: _root_.scala.collection.immutable.VectorBuilder[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor] = new _root_.scala.collection.immutable.VectorBuilder[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor]
    val __rawInputContents: _root_.scala.collection.immutable.VectorBuilder[_root_.com.google.protobuf.ByteString] = new _root_.scala.collection.immutable.VectorBuilder[_root_.com.google.protobuf.ByteString]
    var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
    var _done__ = false
    while (!_done__) {
      val _tag__ = _input__.readTag()
      _tag__ match {
        case 0 => _done__ = true
        case 10 =>
          __modelName = _input__.readStringRequireUtf8()
        case 18 =>
          __modelVersion = _input__.readStringRequireUtf8()
        case 26 =>
          __id = _input__.readStringRequireUtf8()
        case 34 =>
          __parameters += org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest._typemapper_parameters.toCustom(_root_.scalapb.LiteParser.readMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry](_input__))
        case 42 =>
          __inputs += _root_.scalapb.LiteParser.readMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor](_input__)
        case 50 =>
          __outputs += _root_.scalapb.LiteParser.readMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor](_input__)
        case 58 =>
          __rawInputContents += _input__.readBytes()
        case tag =>
          if (_unknownFields__ == null) {
            _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
          }
          _unknownFields__.parseField(tag, _input__)
      }
    }
    org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest(
        modelName = __modelName,
        modelVersion = __modelVersion,
        id = __id,
        parameters = __parameters.result(),
        inputs = __inputs.result(),
        outputs = __outputs.result(),
        rawInputContents = __rawInputContents.result(),
        unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
    )
  }
  implicit def messageReads: _root_.scalapb.descriptors.Reads[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest] = _root_.scalapb.descriptors.Reads{
    case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
      _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
      org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest(
        modelName = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
        modelVersion = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
        id = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
        parameters = __fieldsMap.get(scalaDescriptor.findFieldByNumber(4).get).map(_.as[_root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry]]).getOrElse(_root_.scala.Seq.empty).iterator.map(org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest._typemapper_parameters.toCustom(_)).toMap,
        inputs = __fieldsMap.get(scalaDescriptor.findFieldByNumber(5).get).map(_.as[_root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor]]).getOrElse(_root_.scala.Seq.empty),
        outputs = __fieldsMap.get(scalaDescriptor.findFieldByNumber(6).get).map(_.as[_root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor]]).getOrElse(_root_.scala.Seq.empty),
        rawInputContents = __fieldsMap.get(scalaDescriptor.findFieldByNumber(7).get).map(_.as[_root_.scala.Seq[_root_.com.google.protobuf.ByteString]]).getOrElse(_root_.scala.Seq.empty)
      )
    case _ => throw new RuntimeException("Expected PMessage")
  }
  def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.javaDescriptor.getMessageTypes().get(10)
  def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.scalaDescriptor.messages(10)
  def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?] = {
    var __out: _root_.scalapb.GeneratedMessageCompanion[?] = null
    (__number: @_root_.scala.unchecked) match {
      case 4 => __out = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry
      case 5 => __out = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor
      case 6 => __out = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor
    }
    __out
  }
  lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] =
    Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]](
      _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor,
      _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor,
      _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry
    )
  def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?] = throw new MatchError(__fieldNumber)
  lazy val defaultInstance = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest(
    modelName = "",
    modelVersion = "",
    id = "",
    parameters = _root_.scala.collection.immutable.Map.empty,
    inputs = _root_.scala.Seq.empty,
    outputs = _root_.scala.Seq.empty,
    rawInputContents = _root_.scala.Seq.empty
  )
  /** An input tensor for an inference request.
    *
    * @param name
    *   The tensor name.
    * @param datatype
    *   The tensor data type.
    * @param shape
    *   The tensor shape.
    * @param parameters
    *   Optional inference input tensor parameters.
    * @param contents
    *   The tensor contents using a data-type format. This field must
    *   not be specified if "raw" tensor contents are being used for
    *   the inference request.
    */
  @SerialVersionUID(0L)
  final case class InferInputTensor(
      name: _root_.scala.Predef.String = "",
      datatype: _root_.scala.Predef.String = "",
      shape: _root_.scala.Seq[_root_.scala.Long] = _root_.scala.Seq.empty,
      parameters: _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter] = _root_.scala.collection.immutable.Map.empty,
      contents: _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents] = _root_.scala.None,
      unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
      ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[InferInputTensor] {
      private[this] def shapeSerializedSize = {
        if (__shapeSerializedSizeField == 0) __shapeSerializedSizeField = {
          var __s: _root_.scala.Int = 0
          shape.foreach(__i => __s += _root_.com.google.protobuf.CodedOutputStream.computeInt64SizeNoTag(__i))
          __s
        }
        __shapeSerializedSizeField
      }
      @transient private[this] var __shapeSerializedSizeField: _root_.scala.Int = 0
      @transient
      private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
      private[this] def __computeSerializedSize(): _root_.scala.Int = {
        var __size = 0
        
        {
          val __value = name
          if (!__value.isEmpty) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(1, __value)
          }
        };
        
        {
          val __value = datatype
          if (!__value.isEmpty) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(2, __value)
          }
        };
        if (shape.nonEmpty) {
          val __localsize = shapeSerializedSize
          __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
        }
        parameters.foreach { __item =>
          val __value = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor._typemapper_parameters.toBase(__item)
          __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
        }
        if (contents.isDefined) {
          val __value = contents.get
          __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
        };
        __size += unknownFields.serializedSize
        __size
      }
      override def serializedSize: _root_.scala.Int = {
        var __size = __serializedSizeMemoized
        if (__size == 0) {
          __size = __computeSerializedSize() + 1
          __serializedSizeMemoized = __size
        }
        __size - 1
        
      }
      def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
        {
          val __v = name
          if (!__v.isEmpty) {
            _output__.writeString(1, __v)
          }
        };
        {
          val __v = datatype
          if (!__v.isEmpty) {
            _output__.writeString(2, __v)
          }
        };
        if (shape.nonEmpty) {
          _output__.writeTag(3, 2)
          _output__.writeUInt32NoTag(shapeSerializedSize)
          shape.foreach(_output__.writeInt64NoTag)
        };
        parameters.foreach { __v =>
          val __m = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor._typemapper_parameters.toBase(__v)
          _output__.writeTag(4, 2)
          _output__.writeUInt32NoTag(__m.serializedSize)
          __m.writeTo(_output__)
        };
        contents.foreach { __v =>
          val __m = __v
          _output__.writeTag(5, 2)
          _output__.writeUInt32NoTag(__m.serializedSize)
          __m.writeTo(_output__)
        };
        unknownFields.writeTo(_output__)
      }
      def withName(__v: _root_.scala.Predef.String): InferInputTensor = copy(name = __v)
      def withDatatype(__v: _root_.scala.Predef.String): InferInputTensor = copy(datatype = __v)
      def clearShape = copy(shape = _root_.scala.Seq.empty)
      def addShape(__vs: _root_.scala.Long *): InferInputTensor = addAllShape(__vs)
      def addAllShape(__vs: Iterable[_root_.scala.Long]): InferInputTensor = copy(shape = shape ++ __vs)
      def withShape(__v: _root_.scala.Seq[_root_.scala.Long]): InferInputTensor = copy(shape = __v)
      def clearParameters = copy(parameters = _root_.scala.collection.immutable.Map.empty)
      def addParameters(__vs: (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter) *): InferInputTensor = addAllParameters(__vs)
      def addAllParameters(__vs: Iterable[(_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)]): InferInputTensor = copy(parameters = parameters ++ __vs)
      def withParameters(__v: _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]): InferInputTensor = copy(parameters = __v)
      def getContents: org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents = contents.getOrElse(org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents.defaultInstance)
      def clearContents: InferInputTensor = copy(contents = _root_.scala.None)
      def withContents(__v: org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents): InferInputTensor = copy(contents = Option(__v))
      def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
      def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
      def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
        (__fieldNumber: @_root_.scala.unchecked) match {
          case 1 => {
            val __t = name
            if (__t != "") __t else null
          }
          case 2 => {
            val __t = datatype
            if (__t != "") __t else null
          }
          case 3 => shape
          case 4 => parameters.iterator.map(org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor._typemapper_parameters.toBase(_)).toSeq
          case 5 => contents.orNull
        }
      }
      def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
        _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
        (__field.number: @_root_.scala.unchecked) match {
          case 1 => _root_.scalapb.descriptors.PString(name)
          case 2 => _root_.scalapb.descriptors.PString(datatype)
          case 3 => _root_.scalapb.descriptors.PRepeated(shape.iterator.map(_root_.scalapb.descriptors.PLong(_)).toVector)
          case 4 => _root_.scalapb.descriptors.PRepeated(parameters.iterator.map(org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor._typemapper_parameters.toBase(_).toPMessage).toVector)
          case 5 => contents.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
        }
      }
      def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
      def companion: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.type = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor
      // @@protoc_insertion_point(GeneratedMessage[org.pytorch.serve.grpc.openinference.ModelInferRequest.InferInputTensor])
  }
  
  object InferInputTensor extends scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor] {
    implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor] = this
    def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor = {
      var __name: _root_.scala.Predef.String = ""
      var __datatype: _root_.scala.Predef.String = ""
      val __shape: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Long] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Long]
      val __parameters: _root_.scala.collection.mutable.Builder[(_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter), _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]] = _root_.scala.collection.immutable.Map.newBuilder[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]
      var __contents: _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents] = _root_.scala.None
      var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
      var _done__ = false
      while (!_done__) {
        val _tag__ = _input__.readTag()
        _tag__ match {
          case 0 => _done__ = true
          case 10 =>
            __name = _input__.readStringRequireUtf8()
          case 18 =>
            __datatype = _input__.readStringRequireUtf8()
          case 24 =>
            __shape += _input__.readInt64()
          case 26 => {
            val length = _input__.readRawVarint32()
            val oldLimit = _input__.pushLimit(length)
            while (_input__.getBytesUntilLimit > 0) {
              __shape += _input__.readInt64()
            }
            _input__.popLimit(oldLimit)
          }
          case 34 =>
            __parameters += org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor._typemapper_parameters.toCustom(_root_.scalapb.LiteParser.readMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry](_input__))
          case 42 =>
            __contents = _root_.scala.Option(__contents.fold(_root_.scalapb.LiteParser.readMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
          case tag =>
            if (_unknownFields__ == null) {
              _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
            }
            _unknownFields__.parseField(tag, _input__)
        }
      }
      org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor(
          name = __name,
          datatype = __datatype,
          shape = __shape.result(),
          parameters = __parameters.result(),
          contents = __contents,
          unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
      )
    }
    implicit def messageReads: _root_.scalapb.descriptors.Reads[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor] = _root_.scalapb.descriptors.Reads{
      case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
        _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
        org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor(
          name = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
          datatype = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
          shape = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Seq[_root_.scala.Long]]).getOrElse(_root_.scala.Seq.empty),
          parameters = __fieldsMap.get(scalaDescriptor.findFieldByNumber(4).get).map(_.as[_root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry]]).getOrElse(_root_.scala.Seq.empty).iterator.map(org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor._typemapper_parameters.toCustom(_)).toMap,
          contents = __fieldsMap.get(scalaDescriptor.findFieldByNumber(5).get).flatMap(_.as[_root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents]])
        )
      case _ => throw new RuntimeException("Expected PMessage")
    }
    def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.javaDescriptor.getNestedTypes().get(0)
    def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.scalaDescriptor.nestedMessages(0)
    def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?] = {
      var __out: _root_.scalapb.GeneratedMessageCompanion[?] = null
      (__number: @_root_.scala.unchecked) match {
        case 4 => __out = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry
        case 5 => __out = org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents
      }
      __out
    }
    lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] =
      Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]](
        _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry
      )
    def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?] = throw new MatchError(__fieldNumber)
    lazy val defaultInstance = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor(
      name = "",
      datatype = "",
      shape = _root_.scala.Seq.empty,
      parameters = _root_.scala.collection.immutable.Map.empty,
      contents = _root_.scala.None
    )
    @SerialVersionUID(0L)
    final case class ParametersEntry(
        key: _root_.scala.Predef.String = "",
        value: _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter] = _root_.scala.None,
        unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
        ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[ParametersEntry] {
        @transient
        private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
        private[this] def __computeSerializedSize(): _root_.scala.Int = {
          var __size = 0
          
          {
            val __value = key
            if (!__value.isEmpty) {
              __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(1, __value)
            }
          };
          if (value.isDefined) {
            val __value = value.get
            __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
          };
          __size += unknownFields.serializedSize
          __size
        }
        override def serializedSize: _root_.scala.Int = {
          var __size = __serializedSizeMemoized
          if (__size == 0) {
            __size = __computeSerializedSize() + 1
            __serializedSizeMemoized = __size
          }
          __size - 1
          
        }
        def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
          {
            val __v = key
            if (!__v.isEmpty) {
              _output__.writeString(1, __v)
            }
          };
          value.foreach { __v =>
            val __m = __v
            _output__.writeTag(2, 2)
            _output__.writeUInt32NoTag(__m.serializedSize)
            __m.writeTo(_output__)
          };
          unknownFields.writeTo(_output__)
        }
        def withKey(__v: _root_.scala.Predef.String): ParametersEntry = copy(key = __v)
        def getValue: org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter = value.getOrElse(org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter.defaultInstance)
        def clearValue: ParametersEntry = copy(value = _root_.scala.None)
        def withValue(__v: org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter): ParametersEntry = copy(value = Option(__v))
        def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
        def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
        def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
          (__fieldNumber: @_root_.scala.unchecked) match {
            case 1 => {
              val __t = key
              if (__t != "") __t else null
            }
            case 2 => value.orNull
          }
        }
        def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
          _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
          (__field.number: @_root_.scala.unchecked) match {
            case 1 => _root_.scalapb.descriptors.PString(key)
            case 2 => value.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
          }
        }
        def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
        def companion: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry.type = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry
        // @@protoc_insertion_point(GeneratedMessage[org.pytorch.serve.grpc.openinference.ModelInferRequest.InferInputTensor.ParametersEntry])
    }
    
    object ParametersEntry extends scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry] {
      implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry] = this
      def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry = {
        var __key: _root_.scala.Predef.String = ""
        var __value: _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter] = _root_.scala.None
        var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
        var _done__ = false
        while (!_done__) {
          val _tag__ = _input__.readTag()
          _tag__ match {
            case 0 => _done__ = true
            case 10 =>
              __key = _input__.readStringRequireUtf8()
            case 18 =>
              __value = _root_.scala.Option(__value.fold(_root_.scalapb.LiteParser.readMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
            case tag =>
              if (_unknownFields__ == null) {
                _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
              }
              _unknownFields__.parseField(tag, _input__)
          }
        }
        org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry(
            key = __key,
            value = __value,
            unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
        )
      }
      implicit def messageReads: _root_.scalapb.descriptors.Reads[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry] = _root_.scalapb.descriptors.Reads{
        case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
          _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
          org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry(
            key = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
            value = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).flatMap(_.as[_root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]])
          )
        case _ => throw new RuntimeException("Expected PMessage")
      }
      def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.javaDescriptor.getNestedTypes().get(0)
      def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.scalaDescriptor.nestedMessages(0)
      def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?] = {
        var __out: _root_.scalapb.GeneratedMessageCompanion[?] = null
        (__number: @_root_.scala.unchecked) match {
          case 2 => __out = org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter
        }
        __out
      }
      lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] = Seq.empty
      def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?] = throw new MatchError(__fieldNumber)
      lazy val defaultInstance = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry(
        key = "",
        value = _root_.scala.None
      )
      implicit class ParametersEntryLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry](_l) {
        def key: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.key)((c_, f_) => c_.copy(key = f_))
        def value: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter] = field(_.getValue)((c_, f_) => c_.copy(value = _root_.scala.Option(f_)))
        def optionalValue: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]] = field(_.value)((c_, f_) => c_.copy(value = f_))
      }
      final val KEY_FIELD_NUMBER = 1
      final val VALUE_FIELD_NUMBER = 2
      @transient
      implicit val keyValueMapper: _root_.scalapb.TypeMapper[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry, (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)] =
        _root_.scalapb.TypeMapper[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry, (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)](__m => (__m.key, __m.getValue))(__p => org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry(__p._1, Some(__p._2)))
      def of(
        key: _root_.scala.Predef.String,
        value: _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]
      ): _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry = _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry(
        key,
        value
      )
      // @@protoc_insertion_point(GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.ModelInferRequest.InferInputTensor.ParametersEntry])
    }
    
    implicit class InferInputTensorLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor](_l) {
      def name: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.name)((c_, f_) => c_.copy(name = f_))
      def datatype: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.datatype)((c_, f_) => c_.copy(datatype = f_))
      def shape: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Long]] = field(_.shape)((c_, f_) => c_.copy(shape = f_))
      def parameters: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]] = field(_.parameters)((c_, f_) => c_.copy(parameters = f_))
      def contents: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents] = field(_.getContents)((c_, f_) => c_.copy(contents = _root_.scala.Option(f_)))
      def optionalContents: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents]] = field(_.contents)((c_, f_) => c_.copy(contents = f_))
    }
    final val NAME_FIELD_NUMBER = 1
    final val DATATYPE_FIELD_NUMBER = 2
    final val SHAPE_FIELD_NUMBER = 3
    final val PARAMETERS_FIELD_NUMBER = 4
    final val CONTENTS_FIELD_NUMBER = 5
    @transient
    private[open_inference_grpc] val _typemapper_parameters: _root_.scalapb.TypeMapper[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry, (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)] = implicitly[_root_.scalapb.TypeMapper[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor.ParametersEntry, (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)]]
    def of(
      name: _root_.scala.Predef.String,
      datatype: _root_.scala.Predef.String,
      shape: _root_.scala.Seq[_root_.scala.Long],
      parameters: _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter],
      contents: _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents]
    ): _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor = _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor(
      name,
      datatype,
      shape,
      parameters,
      contents
    )
    // @@protoc_insertion_point(GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.ModelInferRequest.InferInputTensor])
  }
  
  /** An output tensor requested for an inference request.
    *
    * @param name
    *   The tensor name.
    * @param parameters
    *   Optional requested output tensor parameters.
    */
  @SerialVersionUID(0L)
  final case class InferRequestedOutputTensor(
      name: _root_.scala.Predef.String = "",
      parameters: _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter] = _root_.scala.collection.immutable.Map.empty,
      unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
      ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[InferRequestedOutputTensor] {
      @transient
      private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
      private[this] def __computeSerializedSize(): _root_.scala.Int = {
        var __size = 0
        
        {
          val __value = name
          if (!__value.isEmpty) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(1, __value)
          }
        };
        parameters.foreach { __item =>
          val __value = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor._typemapper_parameters.toBase(__item)
          __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
        }
        __size += unknownFields.serializedSize
        __size
      }
      override def serializedSize: _root_.scala.Int = {
        var __size = __serializedSizeMemoized
        if (__size == 0) {
          __size = __computeSerializedSize() + 1
          __serializedSizeMemoized = __size
        }
        __size - 1
        
      }
      def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
        {
          val __v = name
          if (!__v.isEmpty) {
            _output__.writeString(1, __v)
          }
        };
        parameters.foreach { __v =>
          val __m = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor._typemapper_parameters.toBase(__v)
          _output__.writeTag(2, 2)
          _output__.writeUInt32NoTag(__m.serializedSize)
          __m.writeTo(_output__)
        };
        unknownFields.writeTo(_output__)
      }
      def withName(__v: _root_.scala.Predef.String): InferRequestedOutputTensor = copy(name = __v)
      def clearParameters = copy(parameters = _root_.scala.collection.immutable.Map.empty)
      def addParameters(__vs: (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter) *): InferRequestedOutputTensor = addAllParameters(__vs)
      def addAllParameters(__vs: Iterable[(_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)]): InferRequestedOutputTensor = copy(parameters = parameters ++ __vs)
      def withParameters(__v: _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]): InferRequestedOutputTensor = copy(parameters = __v)
      def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
      def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
      def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
        (__fieldNumber: @_root_.scala.unchecked) match {
          case 1 => {
            val __t = name
            if (__t != "") __t else null
          }
          case 2 => parameters.iterator.map(org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor._typemapper_parameters.toBase(_)).toSeq
        }
      }
      def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
        _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
        (__field.number: @_root_.scala.unchecked) match {
          case 1 => _root_.scalapb.descriptors.PString(name)
          case 2 => _root_.scalapb.descriptors.PRepeated(parameters.iterator.map(org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor._typemapper_parameters.toBase(_).toPMessage).toVector)
        }
      }
      def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
      def companion: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.type = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor
      // @@protoc_insertion_point(GeneratedMessage[org.pytorch.serve.grpc.openinference.ModelInferRequest.InferRequestedOutputTensor])
  }
  
  object InferRequestedOutputTensor extends scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor] {
    implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor] = this
    def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor = {
      var __name: _root_.scala.Predef.String = ""
      val __parameters: _root_.scala.collection.mutable.Builder[(_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter), _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]] = _root_.scala.collection.immutable.Map.newBuilder[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]
      var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
      var _done__ = false
      while (!_done__) {
        val _tag__ = _input__.readTag()
        _tag__ match {
          case 0 => _done__ = true
          case 10 =>
            __name = _input__.readStringRequireUtf8()
          case 18 =>
            __parameters += org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor._typemapper_parameters.toCustom(_root_.scalapb.LiteParser.readMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry](_input__))
          case tag =>
            if (_unknownFields__ == null) {
              _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
            }
            _unknownFields__.parseField(tag, _input__)
        }
      }
      org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor(
          name = __name,
          parameters = __parameters.result(),
          unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
      )
    }
    implicit def messageReads: _root_.scalapb.descriptors.Reads[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor] = _root_.scalapb.descriptors.Reads{
      case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
        _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
        org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor(
          name = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
          parameters = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry]]).getOrElse(_root_.scala.Seq.empty).iterator.map(org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor._typemapper_parameters.toCustom(_)).toMap
        )
      case _ => throw new RuntimeException("Expected PMessage")
    }
    def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.javaDescriptor.getNestedTypes().get(1)
    def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.scalaDescriptor.nestedMessages(1)
    def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?] = {
      var __out: _root_.scalapb.GeneratedMessageCompanion[?] = null
      (__number: @_root_.scala.unchecked) match {
        case 2 => __out = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry
      }
      __out
    }
    lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] =
      Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]](
        _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry
      )
    def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?] = throw new MatchError(__fieldNumber)
    lazy val defaultInstance = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor(
      name = "",
      parameters = _root_.scala.collection.immutable.Map.empty
    )
    @SerialVersionUID(0L)
    final case class ParametersEntry(
        key: _root_.scala.Predef.String = "",
        value: _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter] = _root_.scala.None,
        unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
        ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[ParametersEntry] {
        @transient
        private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
        private[this] def __computeSerializedSize(): _root_.scala.Int = {
          var __size = 0
          
          {
            val __value = key
            if (!__value.isEmpty) {
              __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(1, __value)
            }
          };
          if (value.isDefined) {
            val __value = value.get
            __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
          };
          __size += unknownFields.serializedSize
          __size
        }
        override def serializedSize: _root_.scala.Int = {
          var __size = __serializedSizeMemoized
          if (__size == 0) {
            __size = __computeSerializedSize() + 1
            __serializedSizeMemoized = __size
          }
          __size - 1
          
        }
        def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
          {
            val __v = key
            if (!__v.isEmpty) {
              _output__.writeString(1, __v)
            }
          };
          value.foreach { __v =>
            val __m = __v
            _output__.writeTag(2, 2)
            _output__.writeUInt32NoTag(__m.serializedSize)
            __m.writeTo(_output__)
          };
          unknownFields.writeTo(_output__)
        }
        def withKey(__v: _root_.scala.Predef.String): ParametersEntry = copy(key = __v)
        def getValue: org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter = value.getOrElse(org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter.defaultInstance)
        def clearValue: ParametersEntry = copy(value = _root_.scala.None)
        def withValue(__v: org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter): ParametersEntry = copy(value = Option(__v))
        def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
        def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
        def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
          (__fieldNumber: @_root_.scala.unchecked) match {
            case 1 => {
              val __t = key
              if (__t != "") __t else null
            }
            case 2 => value.orNull
          }
        }
        def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
          _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
          (__field.number: @_root_.scala.unchecked) match {
            case 1 => _root_.scalapb.descriptors.PString(key)
            case 2 => value.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
          }
        }
        def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
        def companion: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry.type = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry
        // @@protoc_insertion_point(GeneratedMessage[org.pytorch.serve.grpc.openinference.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry])
    }
    
    object ParametersEntry extends scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry] {
      implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry] = this
      def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry = {
        var __key: _root_.scala.Predef.String = ""
        var __value: _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter] = _root_.scala.None
        var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
        var _done__ = false
        while (!_done__) {
          val _tag__ = _input__.readTag()
          _tag__ match {
            case 0 => _done__ = true
            case 10 =>
              __key = _input__.readStringRequireUtf8()
            case 18 =>
              __value = _root_.scala.Option(__value.fold(_root_.scalapb.LiteParser.readMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
            case tag =>
              if (_unknownFields__ == null) {
                _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
              }
              _unknownFields__.parseField(tag, _input__)
          }
        }
        org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry(
            key = __key,
            value = __value,
            unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
        )
      }
      implicit def messageReads: _root_.scalapb.descriptors.Reads[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry] = _root_.scalapb.descriptors.Reads{
        case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
          _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
          org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry(
            key = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
            value = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).flatMap(_.as[_root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]])
          )
        case _ => throw new RuntimeException("Expected PMessage")
      }
      def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.javaDescriptor.getNestedTypes().get(0)
      def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.scalaDescriptor.nestedMessages(0)
      def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?] = {
        var __out: _root_.scalapb.GeneratedMessageCompanion[?] = null
        (__number: @_root_.scala.unchecked) match {
          case 2 => __out = org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter
        }
        __out
      }
      lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] = Seq.empty
      def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?] = throw new MatchError(__fieldNumber)
      lazy val defaultInstance = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry(
        key = "",
        value = _root_.scala.None
      )
      implicit class ParametersEntryLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry](_l) {
        def key: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.key)((c_, f_) => c_.copy(key = f_))
        def value: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter] = field(_.getValue)((c_, f_) => c_.copy(value = _root_.scala.Option(f_)))
        def optionalValue: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]] = field(_.value)((c_, f_) => c_.copy(value = f_))
      }
      final val KEY_FIELD_NUMBER = 1
      final val VALUE_FIELD_NUMBER = 2
      @transient
      implicit val keyValueMapper: _root_.scalapb.TypeMapper[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry, (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)] =
        _root_.scalapb.TypeMapper[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry, (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)](__m => (__m.key, __m.getValue))(__p => org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry(__p._1, Some(__p._2)))
      def of(
        key: _root_.scala.Predef.String,
        value: _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]
      ): _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry = _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry(
        key,
        value
      )
      // @@protoc_insertion_point(GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry])
    }
    
    implicit class InferRequestedOutputTensorLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor](_l) {
      def name: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.name)((c_, f_) => c_.copy(name = f_))
      def parameters: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]] = field(_.parameters)((c_, f_) => c_.copy(parameters = f_))
    }
    final val NAME_FIELD_NUMBER = 1
    final val PARAMETERS_FIELD_NUMBER = 2
    @transient
    private[open_inference_grpc] val _typemapper_parameters: _root_.scalapb.TypeMapper[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry, (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)] = implicitly[_root_.scalapb.TypeMapper[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry, (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)]]
    def of(
      name: _root_.scala.Predef.String,
      parameters: _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]
    ): _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor = _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor(
      name,
      parameters
    )
    // @@protoc_insertion_point(GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.ModelInferRequest.InferRequestedOutputTensor])
  }
  
  @SerialVersionUID(0L)
  final case class ParametersEntry(
      key: _root_.scala.Predef.String = "",
      value: _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter] = _root_.scala.None,
      unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
      ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[ParametersEntry] {
      @transient
      private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
      private[this] def __computeSerializedSize(): _root_.scala.Int = {
        var __size = 0
        
        {
          val __value = key
          if (!__value.isEmpty) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(1, __value)
          }
        };
        if (value.isDefined) {
          val __value = value.get
          __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
        };
        __size += unknownFields.serializedSize
        __size
      }
      override def serializedSize: _root_.scala.Int = {
        var __size = __serializedSizeMemoized
        if (__size == 0) {
          __size = __computeSerializedSize() + 1
          __serializedSizeMemoized = __size
        }
        __size - 1
        
      }
      def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
        {
          val __v = key
          if (!__v.isEmpty) {
            _output__.writeString(1, __v)
          }
        };
        value.foreach { __v =>
          val __m = __v
          _output__.writeTag(2, 2)
          _output__.writeUInt32NoTag(__m.serializedSize)
          __m.writeTo(_output__)
        };
        unknownFields.writeTo(_output__)
      }
      def withKey(__v: _root_.scala.Predef.String): ParametersEntry = copy(key = __v)
      def getValue: org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter = value.getOrElse(org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter.defaultInstance)
      def clearValue: ParametersEntry = copy(value = _root_.scala.None)
      def withValue(__v: org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter): ParametersEntry = copy(value = Option(__v))
      def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
      def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
      def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
        (__fieldNumber: @_root_.scala.unchecked) match {
          case 1 => {
            val __t = key
            if (__t != "") __t else null
          }
          case 2 => value.orNull
        }
      }
      def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
        _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
        (__field.number: @_root_.scala.unchecked) match {
          case 1 => _root_.scalapb.descriptors.PString(key)
          case 2 => value.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
        }
      }
      def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
      def companion: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry.type = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry
      // @@protoc_insertion_point(GeneratedMessage[org.pytorch.serve.grpc.openinference.ModelInferRequest.ParametersEntry])
  }
  
  object ParametersEntry extends scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry] {
    implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry] = this
    def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry = {
      var __key: _root_.scala.Predef.String = ""
      var __value: _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter] = _root_.scala.None
      var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
      var _done__ = false
      while (!_done__) {
        val _tag__ = _input__.readTag()
        _tag__ match {
          case 0 => _done__ = true
          case 10 =>
            __key = _input__.readStringRequireUtf8()
          case 18 =>
            __value = _root_.scala.Option(__value.fold(_root_.scalapb.LiteParser.readMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
          case tag =>
            if (_unknownFields__ == null) {
              _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
            }
            _unknownFields__.parseField(tag, _input__)
        }
      }
      org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry(
          key = __key,
          value = __value,
          unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
      )
    }
    implicit def messageReads: _root_.scalapb.descriptors.Reads[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry] = _root_.scalapb.descriptors.Reads{
      case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
        _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
        org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry(
          key = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
          value = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).flatMap(_.as[_root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]])
        )
      case _ => throw new RuntimeException("Expected PMessage")
    }
    def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.javaDescriptor.getNestedTypes().get(2)
    def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.scalaDescriptor.nestedMessages(2)
    def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?] = {
      var __out: _root_.scalapb.GeneratedMessageCompanion[?] = null
      (__number: @_root_.scala.unchecked) match {
        case 2 => __out = org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter
      }
      __out
    }
    lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] = Seq.empty
    def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?] = throw new MatchError(__fieldNumber)
    lazy val defaultInstance = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry(
      key = "",
      value = _root_.scala.None
    )
    implicit class ParametersEntryLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry](_l) {
      def key: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.key)((c_, f_) => c_.copy(key = f_))
      def value: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter] = field(_.getValue)((c_, f_) => c_.copy(value = _root_.scala.Option(f_)))
      def optionalValue: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]] = field(_.value)((c_, f_) => c_.copy(value = f_))
    }
    final val KEY_FIELD_NUMBER = 1
    final val VALUE_FIELD_NUMBER = 2
    @transient
    implicit val keyValueMapper: _root_.scalapb.TypeMapper[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry, (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)] =
      _root_.scalapb.TypeMapper[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry, (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)](__m => (__m.key, __m.getValue))(__p => org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry(__p._1, Some(__p._2)))
    def of(
      key: _root_.scala.Predef.String,
      value: _root_.scala.Option[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]
    ): _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry = _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry(
      key,
      value
    )
    // @@protoc_insertion_point(GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.ModelInferRequest.ParametersEntry])
  }
  
  implicit class ModelInferRequestLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest](_l) {
    def modelName: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.modelName)((c_, f_) => c_.copy(modelName = f_))
    def modelVersion: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.modelVersion)((c_, f_) => c_.copy(modelVersion = f_))
    def id: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.id)((c_, f_) => c_.copy(id = f_))
    def parameters: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter]] = field(_.parameters)((c_, f_) => c_.copy(parameters = f_))
    def inputs: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor]] = field(_.inputs)((c_, f_) => c_.copy(inputs = f_))
    def outputs: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor]] = field(_.outputs)((c_, f_) => c_.copy(outputs = f_))
    def rawInputContents: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.com.google.protobuf.ByteString]] = field(_.rawInputContents)((c_, f_) => c_.copy(rawInputContents = f_))
  }
  final val MODEL_NAME_FIELD_NUMBER = 1
  final val MODEL_VERSION_FIELD_NUMBER = 2
  final val ID_FIELD_NUMBER = 3
  final val PARAMETERS_FIELD_NUMBER = 4
  final val INPUTS_FIELD_NUMBER = 5
  final val OUTPUTS_FIELD_NUMBER = 6
  final val RAW_INPUT_CONTENTS_FIELD_NUMBER = 7
  @transient
  private[open_inference_grpc] val _typemapper_parameters: _root_.scalapb.TypeMapper[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry, (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)] = implicitly[_root_.scalapb.TypeMapper[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.ParametersEntry, (_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter)]]
  def of(
    modelName: _root_.scala.Predef.String,
    modelVersion: _root_.scala.Predef.String,
    id: _root_.scala.Predef.String,
    parameters: _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferParameter],
    inputs: _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferInputTensor],
    outputs: _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest.InferRequestedOutputTensor],
    rawInputContents: _root_.scala.Seq[_root_.com.google.protobuf.ByteString]
  ): _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest = _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest(
    modelName,
    modelVersion,
    id,
    parameters,
    inputs,
    outputs,
    rawInputContents
  )
  // @@protoc_insertion_point(GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.ModelInferRequest])
}
