// Generated by the Scala Plugin for the Protocol Buffer Compiler.
// Do not edit!

package org.pytorch.serve.grpc.openinference.open_inference_grpc

object GRPCInferenceServiceGrpc {
  val METHOD_SERVER_LIVE: _root_.io.grpc.MethodDescriptor[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveRequest, org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveResponse] =
    _root_.io.grpc.MethodDescriptor.newBuilder()
      .setType(_root_.io.grpc.MethodDescriptor.MethodType.UNARY)
      .setFullMethodName(_root_.io.grpc.MethodDescriptor.generateFullMethodName("org.pytorch.serve.grpc.openinference.GRPCInferenceService", "ServerLive"))
      .setSampledToLocalTracing(true)
      .setRequestMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveRequest])
      .setResponseMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveResponse])
      .setSchemaDescriptor(_root_.scalapb.grpc.ConcreteProtoMethodDescriptorSupplier.fromMethodDescriptor(org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.javaDescriptor.getServices().get(0).getMethods().get(0)))
      .build()
  
  val METHOD_SERVER_READY: _root_.io.grpc.MethodDescriptor[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyRequest, org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyResponse] =
    _root_.io.grpc.MethodDescriptor.newBuilder()
      .setType(_root_.io.grpc.MethodDescriptor.MethodType.UNARY)
      .setFullMethodName(_root_.io.grpc.MethodDescriptor.generateFullMethodName("org.pytorch.serve.grpc.openinference.GRPCInferenceService", "ServerReady"))
      .setSampledToLocalTracing(true)
      .setRequestMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyRequest])
      .setResponseMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyResponse])
      .setSchemaDescriptor(_root_.scalapb.grpc.ConcreteProtoMethodDescriptorSupplier.fromMethodDescriptor(org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.javaDescriptor.getServices().get(0).getMethods().get(1)))
      .build()
  
  val METHOD_MODEL_READY: _root_.io.grpc.MethodDescriptor[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyRequest, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyResponse] =
    _root_.io.grpc.MethodDescriptor.newBuilder()
      .setType(_root_.io.grpc.MethodDescriptor.MethodType.UNARY)
      .setFullMethodName(_root_.io.grpc.MethodDescriptor.generateFullMethodName("org.pytorch.serve.grpc.openinference.GRPCInferenceService", "ModelReady"))
      .setSampledToLocalTracing(true)
      .setRequestMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyRequest])
      .setResponseMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyResponse])
      .setSchemaDescriptor(_root_.scalapb.grpc.ConcreteProtoMethodDescriptorSupplier.fromMethodDescriptor(org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.javaDescriptor.getServices().get(0).getMethods().get(2)))
      .build()
  
  val METHOD_SERVER_METADATA: _root_.io.grpc.MethodDescriptor[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataRequest, org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataResponse] =
    _root_.io.grpc.MethodDescriptor.newBuilder()
      .setType(_root_.io.grpc.MethodDescriptor.MethodType.UNARY)
      .setFullMethodName(_root_.io.grpc.MethodDescriptor.generateFullMethodName("org.pytorch.serve.grpc.openinference.GRPCInferenceService", "ServerMetadata"))
      .setSampledToLocalTracing(true)
      .setRequestMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataRequest])
      .setResponseMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataResponse])
      .setSchemaDescriptor(_root_.scalapb.grpc.ConcreteProtoMethodDescriptorSupplier.fromMethodDescriptor(org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.javaDescriptor.getServices().get(0).getMethods().get(3)))
      .build()
  
  val METHOD_MODEL_METADATA: _root_.io.grpc.MethodDescriptor[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataRequest, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse] =
    _root_.io.grpc.MethodDescriptor.newBuilder()
      .setType(_root_.io.grpc.MethodDescriptor.MethodType.UNARY)
      .setFullMethodName(_root_.io.grpc.MethodDescriptor.generateFullMethodName("org.pytorch.serve.grpc.openinference.GRPCInferenceService", "ModelMetadata"))
      .setSampledToLocalTracing(true)
      .setRequestMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataRequest])
      .setResponseMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse])
      .setSchemaDescriptor(_root_.scalapb.grpc.ConcreteProtoMethodDescriptorSupplier.fromMethodDescriptor(org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.javaDescriptor.getServices().get(0).getMethods().get(4)))
      .build()
  
  val METHOD_MODEL_INFER: _root_.io.grpc.MethodDescriptor[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferResponse] =
    _root_.io.grpc.MethodDescriptor.newBuilder()
      .setType(_root_.io.grpc.MethodDescriptor.MethodType.UNARY)
      .setFullMethodName(_root_.io.grpc.MethodDescriptor.generateFullMethodName("org.pytorch.serve.grpc.openinference.GRPCInferenceService", "ModelInfer"))
      .setSampledToLocalTracing(true)
      .setRequestMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest])
      .setResponseMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferResponse])
      .setSchemaDescriptor(_root_.scalapb.grpc.ConcreteProtoMethodDescriptorSupplier.fromMethodDescriptor(org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.javaDescriptor.getServices().get(0).getMethods().get(5)))
      .build()
  
  val SERVICE: _root_.io.grpc.ServiceDescriptor =
    _root_.io.grpc.ServiceDescriptor.newBuilder("org.pytorch.serve.grpc.openinference.GRPCInferenceService")
      .setSchemaDescriptor(new _root_.scalapb.grpc.ConcreteProtoFileDescriptorSupplier(org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.javaDescriptor))
      .addMethod(METHOD_SERVER_LIVE)
      .addMethod(METHOD_SERVER_READY)
      .addMethod(METHOD_MODEL_READY)
      .addMethod(METHOD_SERVER_METADATA)
      .addMethod(METHOD_MODEL_METADATA)
      .addMethod(METHOD_MODEL_INFER)
      .build()
  
  /** Inference Server GRPC endpoints.
    */
  trait GRPCInferenceService extends _root_.scalapb.grpc.AbstractService {
    override def serviceCompanion: _root_.scalapb.grpc.ServiceCompanion[GRPCInferenceService] = GRPCInferenceService
    /** The ServerLive API indicates if the inference server is able to receive 
      * and respond to metadata and inference requests.
      */
    def serverLive(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveRequest): scala.concurrent.Future[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveResponse]
    /** The ServerReady API indicates if the server is ready for inferencing.
      */
    def serverReady(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyRequest): scala.concurrent.Future[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyResponse]
    /** The ModelReady API indicates if a specific model is ready for inferencing.
      */
    def modelReady(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyRequest): scala.concurrent.Future[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyResponse]
    /** The ServerMetadata API provides information about the server. Errors are 
      * indicated by the google.rpc.Status returned for the request. The OK code 
      * indicates success and other codes indicate failure.
      */
    def serverMetadata(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataRequest): scala.concurrent.Future[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataResponse]
    /** The per-model metadata API provides information about a model. Errors are 
      * indicated by the google.rpc.Status returned for the request. The OK code 
      * indicates success and other codes indicate failure.
      */
    def modelMetadata(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataRequest): scala.concurrent.Future[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse]
    /** The ModelInfer API performs inference using the specified model. Errors are
      * indicated by the google.rpc.Status returned for the request. The OK code 
      * indicates success and other codes indicate failure.
      */
    def modelInfer(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest): scala.concurrent.Future[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferResponse]
  }
  
  object GRPCInferenceService extends _root_.scalapb.grpc.ServiceCompanion[GRPCInferenceService] {
    implicit def serviceCompanion: _root_.scalapb.grpc.ServiceCompanion[GRPCInferenceService] = this
    def javaDescriptor: _root_.com.google.protobuf.Descriptors.ServiceDescriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.javaDescriptor.getServices().get(0)
    def scalaDescriptor: _root_.scalapb.descriptors.ServiceDescriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.scalaDescriptor.services(0)
    def bindService(serviceImpl: GRPCInferenceService, executionContext: scala.concurrent.ExecutionContext): _root_.io.grpc.ServerServiceDefinition =
      _root_.io.grpc.ServerServiceDefinition.builder(SERVICE)
      .addMethod(
        METHOD_SERVER_LIVE,
        _root_.io.grpc.stub.ServerCalls.asyncUnaryCall((request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveRequest, observer: _root_.io.grpc.stub.StreamObserver[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveResponse]) => {
          serviceImpl.serverLive(request).onComplete(scalapb.grpc.Grpc.completeObserver(observer))(
            executionContext)
        }))
      .addMethod(
        METHOD_SERVER_READY,
        _root_.io.grpc.stub.ServerCalls.asyncUnaryCall((request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyRequest, observer: _root_.io.grpc.stub.StreamObserver[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyResponse]) => {
          serviceImpl.serverReady(request).onComplete(scalapb.grpc.Grpc.completeObserver(observer))(
            executionContext)
        }))
      .addMethod(
        METHOD_MODEL_READY,
        _root_.io.grpc.stub.ServerCalls.asyncUnaryCall((request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyRequest, observer: _root_.io.grpc.stub.StreamObserver[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyResponse]) => {
          serviceImpl.modelReady(request).onComplete(scalapb.grpc.Grpc.completeObserver(observer))(
            executionContext)
        }))
      .addMethod(
        METHOD_SERVER_METADATA,
        _root_.io.grpc.stub.ServerCalls.asyncUnaryCall((request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataRequest, observer: _root_.io.grpc.stub.StreamObserver[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataResponse]) => {
          serviceImpl.serverMetadata(request).onComplete(scalapb.grpc.Grpc.completeObserver(observer))(
            executionContext)
        }))
      .addMethod(
        METHOD_MODEL_METADATA,
        _root_.io.grpc.stub.ServerCalls.asyncUnaryCall((request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataRequest, observer: _root_.io.grpc.stub.StreamObserver[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse]) => {
          serviceImpl.modelMetadata(request).onComplete(scalapb.grpc.Grpc.completeObserver(observer))(
            executionContext)
        }))
      .addMethod(
        METHOD_MODEL_INFER,
        _root_.io.grpc.stub.ServerCalls.asyncUnaryCall((request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest, observer: _root_.io.grpc.stub.StreamObserver[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferResponse]) => {
          serviceImpl.modelInfer(request).onComplete(scalapb.grpc.Grpc.completeObserver(observer))(
            executionContext)
        }))
      .build()
  }
  
  /** Inference Server GRPC endpoints.
    */
  trait GRPCInferenceServiceBlockingClient {
    def serviceCompanion: _root_.scalapb.grpc.ServiceCompanion[GRPCInferenceService] = GRPCInferenceService
    /** The ServerLive API indicates if the inference server is able to receive 
      * and respond to metadata and inference requests.
      */
    def serverLive(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveRequest): org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveResponse
    /** The ServerReady API indicates if the server is ready for inferencing.
      */
    def serverReady(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyRequest): org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyResponse
    /** The ModelReady API indicates if a specific model is ready for inferencing.
      */
    def modelReady(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyRequest): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyResponse
    /** The ServerMetadata API provides information about the server. Errors are 
      * indicated by the google.rpc.Status returned for the request. The OK code 
      * indicates success and other codes indicate failure.
      */
    def serverMetadata(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataRequest): org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataResponse
    /** The per-model metadata API provides information about a model. Errors are 
      * indicated by the google.rpc.Status returned for the request. The OK code 
      * indicates success and other codes indicate failure.
      */
    def modelMetadata(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataRequest): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse
    /** The ModelInfer API performs inference using the specified model. Errors are
      * indicated by the google.rpc.Status returned for the request. The OK code 
      * indicates success and other codes indicate failure.
      */
    def modelInfer(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferResponse
  }
  
  class GRPCInferenceServiceBlockingStub(channel: _root_.io.grpc.Channel, options: _root_.io.grpc.CallOptions = _root_.io.grpc.CallOptions.DEFAULT) extends _root_.io.grpc.stub.AbstractStub[GRPCInferenceServiceBlockingStub](channel, options) with GRPCInferenceServiceBlockingClient {
    /** The ServerLive API indicates if the inference server is able to receive 
      * and respond to metadata and inference requests.
      */
    override def serverLive(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveRequest): org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveResponse = {
      _root_.scalapb.grpc.ClientCalls.blockingUnaryCall(channel, METHOD_SERVER_LIVE, options, request)
    }
    
    /** The ServerReady API indicates if the server is ready for inferencing.
      */
    override def serverReady(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyRequest): org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyResponse = {
      _root_.scalapb.grpc.ClientCalls.blockingUnaryCall(channel, METHOD_SERVER_READY, options, request)
    }
    
    /** The ModelReady API indicates if a specific model is ready for inferencing.
      */
    override def modelReady(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyRequest): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyResponse = {
      _root_.scalapb.grpc.ClientCalls.blockingUnaryCall(channel, METHOD_MODEL_READY, options, request)
    }
    
    /** The ServerMetadata API provides information about the server. Errors are 
      * indicated by the google.rpc.Status returned for the request. The OK code 
      * indicates success and other codes indicate failure.
      */
    override def serverMetadata(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataRequest): org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataResponse = {
      _root_.scalapb.grpc.ClientCalls.blockingUnaryCall(channel, METHOD_SERVER_METADATA, options, request)
    }
    
    /** The per-model metadata API provides information about a model. Errors are 
      * indicated by the google.rpc.Status returned for the request. The OK code 
      * indicates success and other codes indicate failure.
      */
    override def modelMetadata(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataRequest): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse = {
      _root_.scalapb.grpc.ClientCalls.blockingUnaryCall(channel, METHOD_MODEL_METADATA, options, request)
    }
    
    /** The ModelInfer API performs inference using the specified model. Errors are
      * indicated by the google.rpc.Status returned for the request. The OK code 
      * indicates success and other codes indicate failure.
      */
    override def modelInfer(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferResponse = {
      _root_.scalapb.grpc.ClientCalls.blockingUnaryCall(channel, METHOD_MODEL_INFER, options, request)
    }
    
    override def build(channel: _root_.io.grpc.Channel, options: _root_.io.grpc.CallOptions): GRPCInferenceServiceBlockingStub = new GRPCInferenceServiceBlockingStub(channel, options)
  }
  
  class GRPCInferenceServiceStub(channel: _root_.io.grpc.Channel, options: _root_.io.grpc.CallOptions = _root_.io.grpc.CallOptions.DEFAULT) extends _root_.io.grpc.stub.AbstractStub[GRPCInferenceServiceStub](channel, options) with GRPCInferenceService {
    /** The ServerLive API indicates if the inference server is able to receive 
      * and respond to metadata and inference requests.
      */
    override def serverLive(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveRequest): scala.concurrent.Future[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerLiveResponse] = {
      _root_.scalapb.grpc.ClientCalls.asyncUnaryCall(channel, METHOD_SERVER_LIVE, options, request)
    }
    
    /** The ServerReady API indicates if the server is ready for inferencing.
      */
    override def serverReady(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyRequest): scala.concurrent.Future[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerReadyResponse] = {
      _root_.scalapb.grpc.ClientCalls.asyncUnaryCall(channel, METHOD_SERVER_READY, options, request)
    }
    
    /** The ModelReady API indicates if a specific model is ready for inferencing.
      */
    override def modelReady(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyRequest): scala.concurrent.Future[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelReadyResponse] = {
      _root_.scalapb.grpc.ClientCalls.asyncUnaryCall(channel, METHOD_MODEL_READY, options, request)
    }
    
    /** The ServerMetadata API provides information about the server. Errors are 
      * indicated by the google.rpc.Status returned for the request. The OK code 
      * indicates success and other codes indicate failure.
      */
    override def serverMetadata(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataRequest): scala.concurrent.Future[org.pytorch.serve.grpc.openinference.open_inference_grpc.ServerMetadataResponse] = {
      _root_.scalapb.grpc.ClientCalls.asyncUnaryCall(channel, METHOD_SERVER_METADATA, options, request)
    }
    
    /** The per-model metadata API provides information about a model. Errors are 
      * indicated by the google.rpc.Status returned for the request. The OK code 
      * indicates success and other codes indicate failure.
      */
    override def modelMetadata(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataRequest): scala.concurrent.Future[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse] = {
      _root_.scalapb.grpc.ClientCalls.asyncUnaryCall(channel, METHOD_MODEL_METADATA, options, request)
    }
    
    /** The ModelInfer API performs inference using the specified model. Errors are
      * indicated by the google.rpc.Status returned for the request. The OK code 
      * indicates success and other codes indicate failure.
      */
    override def modelInfer(request: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferRequest): scala.concurrent.Future[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelInferResponse] = {
      _root_.scalapb.grpc.ClientCalls.asyncUnaryCall(channel, METHOD_MODEL_INFER, options, request)
    }
    
    override def build(channel: _root_.io.grpc.Channel, options: _root_.io.grpc.CallOptions): GRPCInferenceServiceStub = new GRPCInferenceServiceStub(channel, options)
  }
  
  object GRPCInferenceServiceStub extends _root_.io.grpc.stub.AbstractStub.StubFactory[GRPCInferenceServiceStub] {
    override def newStub(channel: _root_.io.grpc.Channel, options: _root_.io.grpc.CallOptions): GRPCInferenceServiceStub = new GRPCInferenceServiceStub(channel, options)
    
    implicit val stubFactory: _root_.io.grpc.stub.AbstractStub.StubFactory[GRPCInferenceServiceStub] = this
  }
  
  def bindService(serviceImpl: GRPCInferenceService, executionContext: scala.concurrent.ExecutionContext): _root_.io.grpc.ServerServiceDefinition = GRPCInferenceService.bindService(serviceImpl, executionContext)
  
  def blockingStub(channel: _root_.io.grpc.Channel): GRPCInferenceServiceBlockingStub = new GRPCInferenceServiceBlockingStub(channel)
  
  def stub(channel: _root_.io.grpc.Channel): GRPCInferenceServiceStub = new GRPCInferenceServiceStub(channel)
  
  def javaDescriptor: _root_.com.google.protobuf.Descriptors.ServiceDescriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.javaDescriptor.getServices().get(0)
  
}