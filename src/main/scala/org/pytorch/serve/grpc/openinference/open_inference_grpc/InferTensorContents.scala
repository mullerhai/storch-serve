// Generated by the Scala Plugin for the Protocol Buffer Compiler.
// Do not edit!

package org.pytorch.serve.grpc.openinference.open_inference_grpc

/** The data contained in a tensor represented by the repeated type
  * that matches the tensor's data type. Protobuf oneof is not used
  * because oneofs cannot contain repeated fields.
  *
  * @param boolContents
  *   Representation for BOOL data type. The size must match what is
  *   expected by the tensor's shape. The contents must be the flattened,
  *   one-dimensional, row-major order of the tensor elements.
  * @param intContents
  *   Representation for INT8, INT16, and INT32 data types. The size
  *   must match what is expected by the tensor's shape. The contents
  *   must be the flattened, one-dimensional, row-major order of the
  *   tensor elements.
  * @param int64Contents
  *   Representation for INT64 data types. The size must match what
  *   is expected by the tensor's shape. The contents must be the
  *   flattened, one-dimensional, row-major order of the tensor elements.
  * @param uintContents
  *   Representation for UINT8, UINT16, and UINT32 data types. The size
  *   must match what is expected by the tensor's shape. The contents
  *   must be the flattened, one-dimensional, row-major order of the
  *   tensor elements.
  * @param uint64Contents
  *   Representation for UINT64 data types. The size must match what
  *   is expected by the tensor's shape. The contents must be the
  *   flattened, one-dimensional, row-major order of the tensor elements.
  * @param fp32Contents
  *   Representation for FP32 data type. The size must match what is
  *   expected by the tensor's shape. The contents must be the flattened,
  *   one-dimensional, row-major order of the tensor elements.
  * @param fp64Contents
  *   Representation for FP64 data type. The size must match what is
  *   expected by the tensor's shape. The contents must be the flattened,
  *   one-dimensional, row-major order of the tensor elements.
  * @param bytesContents
  *   Representation for BYTES data type. The size must match what is
  *   expected by the tensor's shape. The contents must be the flattened,
  *   one-dimensional, row-major order of the tensor elements.
  */
@SerialVersionUID(0L)
final case class InferTensorContents(
    boolContents: _root_.scala.Seq[_root_.scala.Boolean] = _root_.scala.Seq.empty,
    intContents: _root_.scala.Seq[_root_.scala.Int] = _root_.scala.Seq.empty,
    int64Contents: _root_.scala.Seq[_root_.scala.Long] = _root_.scala.Seq.empty,
    uintContents: _root_.scala.Seq[_root_.scala.Int] = _root_.scala.Seq.empty,
    uint64Contents: _root_.scala.Seq[_root_.scala.Long] = _root_.scala.Seq.empty,
    fp32Contents: _root_.scala.Seq[_root_.scala.Float] = _root_.scala.Seq.empty,
    fp64Contents: _root_.scala.Seq[_root_.scala.Double] = _root_.scala.Seq.empty,
    bytesContents: _root_.scala.Seq[_root_.com.google.protobuf.ByteString] = _root_.scala.Seq.empty,
    unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
    ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[InferTensorContents] {
    private[this] def boolContentsSerializedSize = {
      1 * boolContents.size
    }
    private[this] def intContentsSerializedSize = {
      if (__intContentsSerializedSizeField == 0) __intContentsSerializedSizeField = {
        var __s: _root_.scala.Int = 0
        intContents.foreach(__i => __s += _root_.com.google.protobuf.CodedOutputStream.computeInt32SizeNoTag(__i))
        __s
      }
      __intContentsSerializedSizeField
    }
    @transient private[this] var __intContentsSerializedSizeField: _root_.scala.Int = 0
    private[this] def int64ContentsSerializedSize = {
      if (__int64ContentsSerializedSizeField == 0) __int64ContentsSerializedSizeField = {
        var __s: _root_.scala.Int = 0
        int64Contents.foreach(__i => __s += _root_.com.google.protobuf.CodedOutputStream.computeInt64SizeNoTag(__i))
        __s
      }
      __int64ContentsSerializedSizeField
    }
    @transient private[this] var __int64ContentsSerializedSizeField: _root_.scala.Int = 0
    private[this] def uintContentsSerializedSize = {
      if (__uintContentsSerializedSizeField == 0) __uintContentsSerializedSizeField = {
        var __s: _root_.scala.Int = 0
        uintContents.foreach(__i => __s += _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__i))
        __s
      }
      __uintContentsSerializedSizeField
    }
    @transient private[this] var __uintContentsSerializedSizeField: _root_.scala.Int = 0
    private[this] def uint64ContentsSerializedSize = {
      if (__uint64ContentsSerializedSizeField == 0) __uint64ContentsSerializedSizeField = {
        var __s: _root_.scala.Int = 0
        uint64Contents.foreach(__i => __s += _root_.com.google.protobuf.CodedOutputStream.computeUInt64SizeNoTag(__i))
        __s
      }
      __uint64ContentsSerializedSizeField
    }
    @transient private[this] var __uint64ContentsSerializedSizeField: _root_.scala.Int = 0
    private[this] def fp32ContentsSerializedSize = {
      4 * fp32Contents.size
    }
    private[this] def fp64ContentsSerializedSize = {
      8 * fp64Contents.size
    }
    @transient
    private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
    private[this] def __computeSerializedSize(): _root_.scala.Int = {
      var __size = 0
      if (boolContents.nonEmpty) {
        val __localsize = boolContentsSerializedSize
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
      }
      if (intContents.nonEmpty) {
        val __localsize = intContentsSerializedSize
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
      }
      if (int64Contents.nonEmpty) {
        val __localsize = int64ContentsSerializedSize
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
      }
      if (uintContents.nonEmpty) {
        val __localsize = uintContentsSerializedSize
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
      }
      if (uint64Contents.nonEmpty) {
        val __localsize = uint64ContentsSerializedSize
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
      }
      if (fp32Contents.nonEmpty) {
        val __localsize = fp32ContentsSerializedSize
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
      }
      if (fp64Contents.nonEmpty) {
        val __localsize = fp64ContentsSerializedSize
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
      }
      bytesContents.foreach { __item =>
        val __value = __item
        __size += _root_.com.google.protobuf.CodedOutputStream.computeBytesSize(8, __value)
      }
      __size += unknownFields.serializedSize
      __size
    }
    override def serializedSize: _root_.scala.Int = {
      var __size = __serializedSizeMemoized
      if (__size == 0) {
        __size = __computeSerializedSize() + 1
        __serializedSizeMemoized = __size
      }
      __size - 1
      
    }
    def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
      if (boolContents.nonEmpty) {
        _output__.writeTag(1, 2)
        _output__.writeUInt32NoTag(boolContentsSerializedSize)
        boolContents.foreach(_output__.writeBoolNoTag)
      };
      if (intContents.nonEmpty) {
        _output__.writeTag(2, 2)
        _output__.writeUInt32NoTag(intContentsSerializedSize)
        intContents.foreach(_output__.writeInt32NoTag)
      };
      if (int64Contents.nonEmpty) {
        _output__.writeTag(3, 2)
        _output__.writeUInt32NoTag(int64ContentsSerializedSize)
        int64Contents.foreach(_output__.writeInt64NoTag)
      };
      if (uintContents.nonEmpty) {
        _output__.writeTag(4, 2)
        _output__.writeUInt32NoTag(uintContentsSerializedSize)
        uintContents.foreach(_output__.writeUInt32NoTag)
      };
      if (uint64Contents.nonEmpty) {
        _output__.writeTag(5, 2)
        _output__.writeUInt32NoTag(uint64ContentsSerializedSize)
        uint64Contents.foreach(_output__.writeUInt64NoTag)
      };
      if (fp32Contents.nonEmpty) {
        _output__.writeTag(6, 2)
        _output__.writeUInt32NoTag(fp32ContentsSerializedSize)
        fp32Contents.foreach(_output__.writeFloatNoTag)
      };
      if (fp64Contents.nonEmpty) {
        _output__.writeTag(7, 2)
        _output__.writeUInt32NoTag(fp64ContentsSerializedSize)
        fp64Contents.foreach(_output__.writeDoubleNoTag)
      };
      bytesContents.foreach { __v =>
        val __m = __v
        _output__.writeBytes(8, __m)
      };
      unknownFields.writeTo(_output__)
    }
    def clearBoolContents = copy(boolContents = _root_.scala.Seq.empty)
    def addBoolContents(__vs: _root_.scala.Boolean *): InferTensorContents = addAllBoolContents(__vs)
    def addAllBoolContents(__vs: Iterable[_root_.scala.Boolean]): InferTensorContents = copy(boolContents = boolContents ++ __vs)
    def withBoolContents(__v: _root_.scala.Seq[_root_.scala.Boolean]): InferTensorContents = copy(boolContents = __v)
    def clearIntContents = copy(intContents = _root_.scala.Seq.empty)
    def addIntContents(__vs: _root_.scala.Int *): InferTensorContents = addAllIntContents(__vs)
    def addAllIntContents(__vs: Iterable[_root_.scala.Int]): InferTensorContents = copy(intContents = intContents ++ __vs)
    def withIntContents(__v: _root_.scala.Seq[_root_.scala.Int]): InferTensorContents = copy(intContents = __v)
    def clearInt64Contents = copy(int64Contents = _root_.scala.Seq.empty)
    def addInt64Contents(__vs: _root_.scala.Long *): InferTensorContents = addAllInt64Contents(__vs)
    def addAllInt64Contents(__vs: Iterable[_root_.scala.Long]): InferTensorContents = copy(int64Contents = int64Contents ++ __vs)
    def withInt64Contents(__v: _root_.scala.Seq[_root_.scala.Long]): InferTensorContents = copy(int64Contents = __v)
    def clearUintContents = copy(uintContents = _root_.scala.Seq.empty)
    def addUintContents(__vs: _root_.scala.Int *): InferTensorContents = addAllUintContents(__vs)
    def addAllUintContents(__vs: Iterable[_root_.scala.Int]): InferTensorContents = copy(uintContents = uintContents ++ __vs)
    def withUintContents(__v: _root_.scala.Seq[_root_.scala.Int]): InferTensorContents = copy(uintContents = __v)
    def clearUint64Contents = copy(uint64Contents = _root_.scala.Seq.empty)
    def addUint64Contents(__vs: _root_.scala.Long *): InferTensorContents = addAllUint64Contents(__vs)
    def addAllUint64Contents(__vs: Iterable[_root_.scala.Long]): InferTensorContents = copy(uint64Contents = uint64Contents ++ __vs)
    def withUint64Contents(__v: _root_.scala.Seq[_root_.scala.Long]): InferTensorContents = copy(uint64Contents = __v)
    def clearFp32Contents = copy(fp32Contents = _root_.scala.Seq.empty)
    def addFp32Contents(__vs: _root_.scala.Float *): InferTensorContents = addAllFp32Contents(__vs)
    def addAllFp32Contents(__vs: Iterable[_root_.scala.Float]): InferTensorContents = copy(fp32Contents = fp32Contents ++ __vs)
    def withFp32Contents(__v: _root_.scala.Seq[_root_.scala.Float]): InferTensorContents = copy(fp32Contents = __v)
    def clearFp64Contents = copy(fp64Contents = _root_.scala.Seq.empty)
    def addFp64Contents(__vs: _root_.scala.Double *): InferTensorContents = addAllFp64Contents(__vs)
    def addAllFp64Contents(__vs: Iterable[_root_.scala.Double]): InferTensorContents = copy(fp64Contents = fp64Contents ++ __vs)
    def withFp64Contents(__v: _root_.scala.Seq[_root_.scala.Double]): InferTensorContents = copy(fp64Contents = __v)
    def clearBytesContents = copy(bytesContents = _root_.scala.Seq.empty)
    def addBytesContents(__vs: _root_.com.google.protobuf.ByteString *): InferTensorContents = addAllBytesContents(__vs)
    def addAllBytesContents(__vs: Iterable[_root_.com.google.protobuf.ByteString]): InferTensorContents = copy(bytesContents = bytesContents ++ __vs)
    def withBytesContents(__v: _root_.scala.Seq[_root_.com.google.protobuf.ByteString]): InferTensorContents = copy(bytesContents = __v)
    def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
    def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
    def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
      (__fieldNumber: @_root_.scala.unchecked) match {
        case 1 => boolContents
        case 2 => intContents
        case 3 => int64Contents
        case 4 => uintContents
        case 5 => uint64Contents
        case 6 => fp32Contents
        case 7 => fp64Contents
        case 8 => bytesContents
      }
    }
    def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
      _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
      (__field.number: @_root_.scala.unchecked) match {
        case 1 => _root_.scalapb.descriptors.PRepeated(boolContents.iterator.map(_root_.scalapb.descriptors.PBoolean(_)).toVector)
        case 2 => _root_.scalapb.descriptors.PRepeated(intContents.iterator.map(_root_.scalapb.descriptors.PInt(_)).toVector)
        case 3 => _root_.scalapb.descriptors.PRepeated(int64Contents.iterator.map(_root_.scalapb.descriptors.PLong(_)).toVector)
        case 4 => _root_.scalapb.descriptors.PRepeated(uintContents.iterator.map(_root_.scalapb.descriptors.PInt(_)).toVector)
        case 5 => _root_.scalapb.descriptors.PRepeated(uint64Contents.iterator.map(_root_.scalapb.descriptors.PLong(_)).toVector)
        case 6 => _root_.scalapb.descriptors.PRepeated(fp32Contents.iterator.map(_root_.scalapb.descriptors.PFloat(_)).toVector)
        case 7 => _root_.scalapb.descriptors.PRepeated(fp64Contents.iterator.map(_root_.scalapb.descriptors.PDouble(_)).toVector)
        case 8 => _root_.scalapb.descriptors.PRepeated(bytesContents.iterator.map(_root_.scalapb.descriptors.PByteString(_)).toVector)
      }
    }
    def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
    def companion: org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents.type = org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents
    // @@protoc_insertion_point(GeneratedMessage[org.pytorch.serve.grpc.openinference.InferTensorContents])
}

object InferTensorContents extends scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents] {
  implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents] = this
  def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents = {
    val __boolContents: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Boolean] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Boolean]
    val __intContents: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Int] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Int]
    val __int64Contents: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Long] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Long]
    val __uintContents: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Int] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Int]
    val __uint64Contents: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Long] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Long]
    val __fp32Contents: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Float] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Float]
    val __fp64Contents: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Double] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Double]
    val __bytesContents: _root_.scala.collection.immutable.VectorBuilder[_root_.com.google.protobuf.ByteString] = new _root_.scala.collection.immutable.VectorBuilder[_root_.com.google.protobuf.ByteString]
    var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
    var _done__ = false
    while (!_done__) {
      val _tag__ = _input__.readTag()
      _tag__ match {
        case 0 => _done__ = true
        case 8 =>
          __boolContents += _input__.readBool()
        case 10 => {
          val length = _input__.readRawVarint32()
          val oldLimit = _input__.pushLimit(length)
          while (_input__.getBytesUntilLimit > 0) {
            __boolContents += _input__.readBool()
          }
          _input__.popLimit(oldLimit)
        }
        case 16 =>
          __intContents += _input__.readInt32()
        case 18 => {
          val length = _input__.readRawVarint32()
          val oldLimit = _input__.pushLimit(length)
          while (_input__.getBytesUntilLimit > 0) {
            __intContents += _input__.readInt32()
          }
          _input__.popLimit(oldLimit)
        }
        case 24 =>
          __int64Contents += _input__.readInt64()
        case 26 => {
          val length = _input__.readRawVarint32()
          val oldLimit = _input__.pushLimit(length)
          while (_input__.getBytesUntilLimit > 0) {
            __int64Contents += _input__.readInt64()
          }
          _input__.popLimit(oldLimit)
        }
        case 32 =>
          __uintContents += _input__.readUInt32()
        case 34 => {
          val length = _input__.readRawVarint32()
          val oldLimit = _input__.pushLimit(length)
          while (_input__.getBytesUntilLimit > 0) {
            __uintContents += _input__.readUInt32()
          }
          _input__.popLimit(oldLimit)
        }
        case 40 =>
          __uint64Contents += _input__.readUInt64()
        case 42 => {
          val length = _input__.readRawVarint32()
          val oldLimit = _input__.pushLimit(length)
          while (_input__.getBytesUntilLimit > 0) {
            __uint64Contents += _input__.readUInt64()
          }
          _input__.popLimit(oldLimit)
        }
        case 53 =>
          __fp32Contents += _input__.readFloat()
        case 50 => {
          val length = _input__.readRawVarint32()
          val oldLimit = _input__.pushLimit(length)
          while (_input__.getBytesUntilLimit > 0) {
            __fp32Contents += _input__.readFloat()
          }
          _input__.popLimit(oldLimit)
        }
        case 57 =>
          __fp64Contents += _input__.readDouble()
        case 58 => {
          val length = _input__.readRawVarint32()
          val oldLimit = _input__.pushLimit(length)
          while (_input__.getBytesUntilLimit > 0) {
            __fp64Contents += _input__.readDouble()
          }
          _input__.popLimit(oldLimit)
        }
        case 66 =>
          __bytesContents += _input__.readBytes()
        case tag =>
          if (_unknownFields__ == null) {
            _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
          }
          _unknownFields__.parseField(tag, _input__)
      }
    }
    org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents(
        boolContents = __boolContents.result(),
        intContents = __intContents.result(),
        int64Contents = __int64Contents.result(),
        uintContents = __uintContents.result(),
        uint64Contents = __uint64Contents.result(),
        fp32Contents = __fp32Contents.result(),
        fp64Contents = __fp64Contents.result(),
        bytesContents = __bytesContents.result(),
        unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
    )
  }
  implicit def messageReads: _root_.scalapb.descriptors.Reads[org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents] = _root_.scalapb.descriptors.Reads{
    case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
      _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
      org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents(
        boolContents = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Seq[_root_.scala.Boolean]]).getOrElse(_root_.scala.Seq.empty),
        intContents = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Seq[_root_.scala.Int]]).getOrElse(_root_.scala.Seq.empty),
        int64Contents = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Seq[_root_.scala.Long]]).getOrElse(_root_.scala.Seq.empty),
        uintContents = __fieldsMap.get(scalaDescriptor.findFieldByNumber(4).get).map(_.as[_root_.scala.Seq[_root_.scala.Int]]).getOrElse(_root_.scala.Seq.empty),
        uint64Contents = __fieldsMap.get(scalaDescriptor.findFieldByNumber(5).get).map(_.as[_root_.scala.Seq[_root_.scala.Long]]).getOrElse(_root_.scala.Seq.empty),
        fp32Contents = __fieldsMap.get(scalaDescriptor.findFieldByNumber(6).get).map(_.as[_root_.scala.Seq[_root_.scala.Float]]).getOrElse(_root_.scala.Seq.empty),
        fp64Contents = __fieldsMap.get(scalaDescriptor.findFieldByNumber(7).get).map(_.as[_root_.scala.Seq[_root_.scala.Double]]).getOrElse(_root_.scala.Seq.empty),
        bytesContents = __fieldsMap.get(scalaDescriptor.findFieldByNumber(8).get).map(_.as[_root_.scala.Seq[_root_.com.google.protobuf.ByteString]]).getOrElse(_root_.scala.Seq.empty)
      )
    case _ => throw new RuntimeException("Expected PMessage")
  }
  def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.javaDescriptor.getMessageTypes().get(13)
  def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.scalaDescriptor.messages(13)
  def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?] = throw new MatchError(__number)
  lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] = Seq.empty
  def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?] = throw new MatchError(__fieldNumber)
  lazy val defaultInstance = org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents(
    boolContents = _root_.scala.Seq.empty,
    intContents = _root_.scala.Seq.empty,
    int64Contents = _root_.scala.Seq.empty,
    uintContents = _root_.scala.Seq.empty,
    uint64Contents = _root_.scala.Seq.empty,
    fp32Contents = _root_.scala.Seq.empty,
    fp64Contents = _root_.scala.Seq.empty,
    bytesContents = _root_.scala.Seq.empty
  )
  implicit class InferTensorContentsLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents](_l) {
    def boolContents: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Boolean]] = field(_.boolContents)((c_, f_) => c_.copy(boolContents = f_))
    def intContents: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Int]] = field(_.intContents)((c_, f_) => c_.copy(intContents = f_))
    def int64Contents: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Long]] = field(_.int64Contents)((c_, f_) => c_.copy(int64Contents = f_))
    def uintContents: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Int]] = field(_.uintContents)((c_, f_) => c_.copy(uintContents = f_))
    def uint64Contents: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Long]] = field(_.uint64Contents)((c_, f_) => c_.copy(uint64Contents = f_))
    def fp32Contents: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Float]] = field(_.fp32Contents)((c_, f_) => c_.copy(fp32Contents = f_))
    def fp64Contents: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Double]] = field(_.fp64Contents)((c_, f_) => c_.copy(fp64Contents = f_))
    def bytesContents: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.com.google.protobuf.ByteString]] = field(_.bytesContents)((c_, f_) => c_.copy(bytesContents = f_))
  }
  final val BOOL_CONTENTS_FIELD_NUMBER = 1
  final val INT_CONTENTS_FIELD_NUMBER = 2
  final val INT64_CONTENTS_FIELD_NUMBER = 3
  final val UINT_CONTENTS_FIELD_NUMBER = 4
  final val UINT64_CONTENTS_FIELD_NUMBER = 5
  final val FP32_CONTENTS_FIELD_NUMBER = 6
  final val FP64_CONTENTS_FIELD_NUMBER = 7
  final val BYTES_CONTENTS_FIELD_NUMBER = 8
  def of(
    boolContents: _root_.scala.Seq[_root_.scala.Boolean],
    intContents: _root_.scala.Seq[_root_.scala.Int],
    int64Contents: _root_.scala.Seq[_root_.scala.Long],
    uintContents: _root_.scala.Seq[_root_.scala.Int],
    uint64Contents: _root_.scala.Seq[_root_.scala.Long],
    fp32Contents: _root_.scala.Seq[_root_.scala.Float],
    fp64Contents: _root_.scala.Seq[_root_.scala.Double],
    bytesContents: _root_.scala.Seq[_root_.com.google.protobuf.ByteString]
  ): _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents = _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.InferTensorContents(
    boolContents,
    intContents,
    int64Contents,
    uintContents,
    uint64Contents,
    fp32Contents,
    fp64Contents,
    bytesContents
  )
  // @@protoc_insertion_point(GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.InferTensorContents])
}
