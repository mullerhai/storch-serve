// Generated by the Scala Plugin for the Protocol Buffer Compiler.
// Do not edit!

package org.pytorch.serve.grpc.openinference.open_inference_grpc

/** @param name
  *   The model name.
  * @param versions
  *   The versions of the model available on the server.
  * @param platform
  *   The model's platform. See Platforms.
  * @param inputs
  *   The model's inputs.
  * @param outputs
  *   The model's outputs.
  */
@SerialVersionUID(0L)
final case class ModelMetadataResponse(
    name: _root_.scala.Predef.String = "",
    versions: _root_.scala.Seq[_root_.scala.Predef.String] = _root_.scala.Seq.empty,
    platform: _root_.scala.Predef.String = "",
    inputs: _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata] = _root_.scala.Seq.empty,
    outputs: _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata] = _root_.scala.Seq.empty,
    unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
    ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[ModelMetadataResponse] {
    @transient
    private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
    private[this] def __computeSerializedSize(): _root_.scala.Int = {
      var __size = 0
      
      {
        val __value = name
        if (!__value.isEmpty) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(1, __value)
        }
      };
      versions.foreach { __item =>
        val __value = __item
        __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(2, __value)
      }
      
      {
        val __value = platform
        if (!__value.isEmpty) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(3, __value)
        }
      };
      inputs.foreach { __item =>
        val __value = __item
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      }
      outputs.foreach { __item =>
        val __value = __item
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      }
      __size += unknownFields.serializedSize
      __size
    }
    override def serializedSize: _root_.scala.Int = {
      var __size = __serializedSizeMemoized
      if (__size == 0) {
        __size = __computeSerializedSize() + 1
        __serializedSizeMemoized = __size
      }
      __size - 1
      
    }
    def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
      {
        val __v = name
        if (!__v.isEmpty) {
          _output__.writeString(1, __v)
        }
      };
      versions.foreach { __v =>
        val __m = __v
        _output__.writeString(2, __m)
      };
      {
        val __v = platform
        if (!__v.isEmpty) {
          _output__.writeString(3, __v)
        }
      };
      inputs.foreach { __v =>
        val __m = __v
        _output__.writeTag(4, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      outputs.foreach { __v =>
        val __m = __v
        _output__.writeTag(5, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      unknownFields.writeTo(_output__)
    }
    def withName(__v: _root_.scala.Predef.String): ModelMetadataResponse = copy(name = __v)
    def clearVersions = copy(versions = _root_.scala.Seq.empty)
    def addVersions(__vs: _root_.scala.Predef.String *): ModelMetadataResponse = addAllVersions(__vs)
    def addAllVersions(__vs: Iterable[_root_.scala.Predef.String]): ModelMetadataResponse = copy(versions = versions ++ __vs)
    def withVersions(__v: _root_.scala.Seq[_root_.scala.Predef.String]): ModelMetadataResponse = copy(versions = __v)
    def withPlatform(__v: _root_.scala.Predef.String): ModelMetadataResponse = copy(platform = __v)
    def clearInputs = copy(inputs = _root_.scala.Seq.empty)
    def addInputs(__vs: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata *): ModelMetadataResponse = addAllInputs(__vs)
    def addAllInputs(__vs: Iterable[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata]): ModelMetadataResponse = copy(inputs = inputs ++ __vs)
    def withInputs(__v: _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata]): ModelMetadataResponse = copy(inputs = __v)
    def clearOutputs = copy(outputs = _root_.scala.Seq.empty)
    def addOutputs(__vs: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata *): ModelMetadataResponse = addAllOutputs(__vs)
    def addAllOutputs(__vs: Iterable[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata]): ModelMetadataResponse = copy(outputs = outputs ++ __vs)
    def withOutputs(__v: _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata]): ModelMetadataResponse = copy(outputs = __v)
    def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
    def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
    def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
      (__fieldNumber: @_root_.scala.unchecked) match {
        case 1 => {
          val __t = name
          if (__t != "") __t else null
        }
        case 2 => versions
        case 3 => {
          val __t = platform
          if (__t != "") __t else null
        }
        case 4 => inputs
        case 5 => outputs
      }
    }
    def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
      _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
      (__field.number: @_root_.scala.unchecked) match {
        case 1 => _root_.scalapb.descriptors.PString(name)
        case 2 => _root_.scalapb.descriptors.PRepeated(versions.iterator.map(_root_.scalapb.descriptors.PString(_)).toVector)
        case 3 => _root_.scalapb.descriptors.PString(platform)
        case 4 => _root_.scalapb.descriptors.PRepeated(inputs.iterator.map(_.toPMessage).toVector)
        case 5 => _root_.scalapb.descriptors.PRepeated(outputs.iterator.map(_.toPMessage).toVector)
      }
    }
    def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
    def companion: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.type = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse
    // @@protoc_insertion_point(GeneratedMessage[org.pytorch.serve.grpc.openinference.ModelMetadataResponse])
}

object ModelMetadataResponse extends scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse] {
  implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse] = this
  def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse = {
    var __name: _root_.scala.Predef.String = ""
    val __versions: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Predef.String] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Predef.String]
    var __platform: _root_.scala.Predef.String = ""
    val __inputs: _root_.scala.collection.immutable.VectorBuilder[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata] = new _root_.scala.collection.immutable.VectorBuilder[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata]
    val __outputs: _root_.scala.collection.immutable.VectorBuilder[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata] = new _root_.scala.collection.immutable.VectorBuilder[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata]
    var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
    var _done__ = false
    while (!_done__) {
      val _tag__ = _input__.readTag()
      _tag__ match {
        case 0 => _done__ = true
        case 10 =>
          __name = _input__.readStringRequireUtf8()
        case 18 =>
          __versions += _input__.readStringRequireUtf8()
        case 26 =>
          __platform = _input__.readStringRequireUtf8()
        case 34 =>
          __inputs += _root_.scalapb.LiteParser.readMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata](_input__)
        case 42 =>
          __outputs += _root_.scalapb.LiteParser.readMessage[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata](_input__)
        case tag =>
          if (_unknownFields__ == null) {
            _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
          }
          _unknownFields__.parseField(tag, _input__)
      }
    }
    org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse(
        name = __name,
        versions = __versions.result(),
        platform = __platform,
        inputs = __inputs.result(),
        outputs = __outputs.result(),
        unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
    )
  }
  implicit def messageReads: _root_.scalapb.descriptors.Reads[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse] = _root_.scalapb.descriptors.Reads{
    case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
      _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
      org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse(
        name = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
        versions = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Seq[_root_.scala.Predef.String]]).getOrElse(_root_.scala.Seq.empty),
        platform = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
        inputs = __fieldsMap.get(scalaDescriptor.findFieldByNumber(4).get).map(_.as[_root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata]]).getOrElse(_root_.scala.Seq.empty),
        outputs = __fieldsMap.get(scalaDescriptor.findFieldByNumber(5).get).map(_.as[_root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata]]).getOrElse(_root_.scala.Seq.empty)
      )
    case _ => throw new RuntimeException("Expected PMessage")
  }
  def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.javaDescriptor.getMessageTypes().get(9)
  def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.OpenInferenceGrpcProto.scalaDescriptor.messages(9)
  def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?] = {
    var __out: _root_.scalapb.GeneratedMessageCompanion[?] = null
    (__number: @_root_.scala.unchecked) match {
      case 4 => __out = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata
      case 5 => __out = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata
    }
    __out
  }
  lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] =
    Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]](
      _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata
    )
  def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?] = throw new MatchError(__fieldNumber)
  lazy val defaultInstance = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse(
    name = "",
    versions = _root_.scala.Seq.empty,
    platform = "",
    inputs = _root_.scala.Seq.empty,
    outputs = _root_.scala.Seq.empty
  )
  /** Metadata for a tensor.
    *
    * @param name
    *   The tensor name.
    * @param datatype
    *   The tensor data type.
    * @param shape
    *   The tensor shape. A variable-size dimension is represented
    *   by a -1 value.
    */
  @SerialVersionUID(0L)
  final case class TensorMetadata(
      name: _root_.scala.Predef.String = "",
      datatype: _root_.scala.Predef.String = "",
      shape: _root_.scala.Seq[_root_.scala.Long] = _root_.scala.Seq.empty,
      unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
      ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[TensorMetadata] {
      private[this] def shapeSerializedSize = {
        if (__shapeSerializedSizeField == 0) __shapeSerializedSizeField = {
          var __s: _root_.scala.Int = 0
          shape.foreach(__i => __s += _root_.com.google.protobuf.CodedOutputStream.computeInt64SizeNoTag(__i))
          __s
        }
        __shapeSerializedSizeField
      }
      @transient private[this] var __shapeSerializedSizeField: _root_.scala.Int = 0
      @transient
      private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
      private[this] def __computeSerializedSize(): _root_.scala.Int = {
        var __size = 0
        
        {
          val __value = name
          if (!__value.isEmpty) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(1, __value)
          }
        };
        
        {
          val __value = datatype
          if (!__value.isEmpty) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(2, __value)
          }
        };
        if (shape.nonEmpty) {
          val __localsize = shapeSerializedSize
          __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
        }
        __size += unknownFields.serializedSize
        __size
      }
      override def serializedSize: _root_.scala.Int = {
        var __size = __serializedSizeMemoized
        if (__size == 0) {
          __size = __computeSerializedSize() + 1
          __serializedSizeMemoized = __size
        }
        __size - 1
        
      }
      def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
        {
          val __v = name
          if (!__v.isEmpty) {
            _output__.writeString(1, __v)
          }
        };
        {
          val __v = datatype
          if (!__v.isEmpty) {
            _output__.writeString(2, __v)
          }
        };
        if (shape.nonEmpty) {
          _output__.writeTag(3, 2)
          _output__.writeUInt32NoTag(shapeSerializedSize)
          shape.foreach(_output__.writeInt64NoTag)
        };
        unknownFields.writeTo(_output__)
      }
      def withName(__v: _root_.scala.Predef.String): TensorMetadata = copy(name = __v)
      def withDatatype(__v: _root_.scala.Predef.String): TensorMetadata = copy(datatype = __v)
      def clearShape = copy(shape = _root_.scala.Seq.empty)
      def addShape(__vs: _root_.scala.Long *): TensorMetadata = addAllShape(__vs)
      def addAllShape(__vs: Iterable[_root_.scala.Long]): TensorMetadata = copy(shape = shape ++ __vs)
      def withShape(__v: _root_.scala.Seq[_root_.scala.Long]): TensorMetadata = copy(shape = __v)
      def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
      def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
      def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
        (__fieldNumber: @_root_.scala.unchecked) match {
          case 1 => {
            val __t = name
            if (__t != "") __t else null
          }
          case 2 => {
            val __t = datatype
            if (__t != "") __t else null
          }
          case 3 => shape
        }
      }
      def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
        _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
        (__field.number: @_root_.scala.unchecked) match {
          case 1 => _root_.scalapb.descriptors.PString(name)
          case 2 => _root_.scalapb.descriptors.PString(datatype)
          case 3 => _root_.scalapb.descriptors.PRepeated(shape.iterator.map(_root_.scalapb.descriptors.PLong(_)).toVector)
        }
      }
      def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
      def companion: org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata.type = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata
      // @@protoc_insertion_point(GeneratedMessage[org.pytorch.serve.grpc.openinference.ModelMetadataResponse.TensorMetadata])
  }
  
  object TensorMetadata extends scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata] {
    implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata] = this
    def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata = {
      var __name: _root_.scala.Predef.String = ""
      var __datatype: _root_.scala.Predef.String = ""
      val __shape: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Long] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Long]
      var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
      var _done__ = false
      while (!_done__) {
        val _tag__ = _input__.readTag()
        _tag__ match {
          case 0 => _done__ = true
          case 10 =>
            __name = _input__.readStringRequireUtf8()
          case 18 =>
            __datatype = _input__.readStringRequireUtf8()
          case 24 =>
            __shape += _input__.readInt64()
          case 26 => {
            val length = _input__.readRawVarint32()
            val oldLimit = _input__.pushLimit(length)
            while (_input__.getBytesUntilLimit > 0) {
              __shape += _input__.readInt64()
            }
            _input__.popLimit(oldLimit)
          }
          case tag =>
            if (_unknownFields__ == null) {
              _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
            }
            _unknownFields__.parseField(tag, _input__)
        }
      }
      org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata(
          name = __name,
          datatype = __datatype,
          shape = __shape.result(),
          unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
      )
    }
    implicit def messageReads: _root_.scalapb.descriptors.Reads[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata] = _root_.scalapb.descriptors.Reads{
      case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
        _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
        org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata(
          name = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
          datatype = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
          shape = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Seq[_root_.scala.Long]]).getOrElse(_root_.scala.Seq.empty)
        )
      case _ => throw new RuntimeException("Expected PMessage")
    }
    def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.javaDescriptor.getNestedTypes().get(0)
    def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.scalaDescriptor.nestedMessages(0)
    def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?] = throw new MatchError(__number)
    lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] = Seq.empty
    def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?] = throw new MatchError(__fieldNumber)
    lazy val defaultInstance = org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata(
      name = "",
      datatype = "",
      shape = _root_.scala.Seq.empty
    )
    implicit class TensorMetadataLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata](_l) {
      def name: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.name)((c_, f_) => c_.copy(name = f_))
      def datatype: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.datatype)((c_, f_) => c_.copy(datatype = f_))
      def shape: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Long]] = field(_.shape)((c_, f_) => c_.copy(shape = f_))
    }
    final val NAME_FIELD_NUMBER = 1
    final val DATATYPE_FIELD_NUMBER = 2
    final val SHAPE_FIELD_NUMBER = 3
    def of(
      name: _root_.scala.Predef.String,
      datatype: _root_.scala.Predef.String,
      shape: _root_.scala.Seq[_root_.scala.Long]
    ): _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata = _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata(
      name,
      datatype,
      shape
    )
    // @@protoc_insertion_point(GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.ModelMetadataResponse.TensorMetadata])
  }
  
  implicit class ModelMetadataResponseLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse](_l) {
    def name: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.name)((c_, f_) => c_.copy(name = f_))
    def versions: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Predef.String]] = field(_.versions)((c_, f_) => c_.copy(versions = f_))
    def platform: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.platform)((c_, f_) => c_.copy(platform = f_))
    def inputs: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata]] = field(_.inputs)((c_, f_) => c_.copy(inputs = f_))
    def outputs: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata]] = field(_.outputs)((c_, f_) => c_.copy(outputs = f_))
  }
  final val NAME_FIELD_NUMBER = 1
  final val VERSIONS_FIELD_NUMBER = 2
  final val PLATFORM_FIELD_NUMBER = 3
  final val INPUTS_FIELD_NUMBER = 4
  final val OUTPUTS_FIELD_NUMBER = 5
  def of(
    name: _root_.scala.Predef.String,
    versions: _root_.scala.Seq[_root_.scala.Predef.String],
    platform: _root_.scala.Predef.String,
    inputs: _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata],
    outputs: _root_.scala.Seq[org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse.TensorMetadata]
  ): _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse = _root_.org.pytorch.serve.grpc.openinference.open_inference_grpc.ModelMetadataResponse(
    name,
    versions,
    platform,
    inputs,
    outputs
  )
  // @@protoc_insertion_point(GeneratedMessageCompanion[org.pytorch.serve.grpc.openinference.ModelMetadataResponse])
}
